{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Deep Hedging\n",
    "\n",
    "Train and evaluate baseline dense network for deep hedging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from src.utils.config import load_config, get_device\n",
    "from src.models.deep_hedging import DeepHedgingNetwork, create_model\n",
    "from src.models.losses import create_loss_function\n",
    "from src.models.trainer import Trainer\n",
    "from src.data.heston import get_or_generate_dataset\n",
    "from src.data.preprocessor import create_dataloaders, compute_features\n",
    "from src.evaluation.metrics import compute_all_metrics, print_metrics\n",
    "from src.evaluation.baselines import DeltaHedgingBaseline, evaluate_all_baselines\n",
    "from src.utils.visualization import plot_pnl_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load Configuration and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config('../configs/config.yaml')\n",
    "device = get_device(config)\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Extract key parameters\n",
    "heston_config = config['data']['heston']\n",
    "K = heston_config['K']\n",
    "T = config['data']['T']\n",
    "n_steps = config['data']['n_steps']\n",
    "dt = T / n_steps\n",
    "\n",
    "print(f\"K={K}, T={T}, n_steps={n_steps}, dt={dt:.6f}\")\n",
    "\n",
    "# Load/generate data using caching system\n",
    "cache_dir = config.get('caching', {}).get('directory', 'cache')\n",
    "\n",
    "S_train, v_train, Z_train = get_or_generate_dataset(config, 'train', cache_dir)\n",
    "S_val, v_val, Z_val = get_or_generate_dataset(config, 'val', cache_dir)\n",
    "S_test, v_test, Z_test = get_or_generate_dataset(config, 'test', cache_dir)\n",
    "\n",
    "print(f\"Data shapes: Train={S_train.shape}, Val={S_val.shape}, Test={S_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = config.get('training', {}).get('batch_size', 256)\n",
    "\n",
    "train_loader, val_loader, test_loader = create_dataloaders(\n",
    "    S_train, v_train, Z_train,\n",
    "    S_val, v_val, Z_val,\n",
    "    S_test, v_test, Z_test,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "print(f\"DataLoaders created with batch_size={batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create Model and Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(config)\n",
    "loss_fn = create_loss_function(config)\n",
    "\n",
    "print(f\"Model architecture:\")\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {model.get_num_parameters():,}\")\n",
    "print(f\"Loss function: {loss_fn}\")\n",
    "\n",
    "# Save initial weights\n",
    "exp_dir = Path('../experiments/baseline')\n",
    "exp_dir.mkdir(parents=True, exist_ok=True)\n",
    "torch.save(model.state_dict(), exp_dir / 'init_weights.pt')\n",
    "print(\"Initial weights saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    loss_fn=loss_fn,\n",
    "    config=config,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Set checkpoint directory\n",
    "trainer.checkpoint_dir = str(exp_dir / 'checkpoints')\n",
    "Path(trainer.checkpoint_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Starting training...\")\n",
    "training_results = trainer.train(train_loader, val_loader)\n",
    "print(f\"Training complete. Best validation loss: {training_results['best_val_loss']:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Plot Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = trainer.training_history\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Loss curves\n",
    "epochs = [h['epoch'] for h in history]\n",
    "train_loss = [h['train_loss'] for h in history]\n",
    "val_loss = [h['val_loss'] for h in history]\n",
    "\n",
    "axes[0].plot(epochs, train_loss, label='Train')\n",
    "axes[0].plot(epochs, val_loss, label='Validation')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training & Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# CVaR evolution\n",
    "val_cvar = [h.get('val_cvar', 0) for h in history]\n",
    "axes[1].plot(epochs, val_cvar, color='red')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('CVaR')\n",
    "axes[1].set_title('Validation CVaR')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Premium evolution\n",
    "val_premium = [h.get('val_premium', 0) for h in history]\n",
    "axes[2].plot(epochs, val_premium, color='green')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('Premium (y)')\n",
    "axes[2].set_title('Learned Premium')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/baseline_training_curves.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "trainer.load_checkpoint('best')\n",
    "model.eval()\n",
    "\n",
    "# Compute P&L on test set\n",
    "all_pnl = []\n",
    "all_deltas = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for S, v, Z in test_loader:\n",
    "        S = S.to(device)\n",
    "        v = v.to(device)\n",
    "        Z = Z.to(device)\n",
    "        \n",
    "        features = compute_features(S, v, K, T, dt)\n",
    "        deltas, y = model(features, S)\n",
    "        pnl = loss_fn.compute_pnl(deltas, S, Z, dt)\n",
    "        \n",
    "        all_pnl.append(pnl.cpu())\n",
    "        all_deltas.append(deltas.cpu())\n",
    "\n",
    "all_pnl = torch.cat(all_pnl).numpy()\n",
    "all_deltas = torch.cat(all_deltas).numpy()\n",
    "\n",
    "# Compute metrics\n",
    "metrics = compute_all_metrics(all_pnl)\n",
    "metrics['learned_premium'] = float(model.y.item())\n",
    "\n",
    "print_metrics(metrics, \"Test Set Metrics\")\n",
    "\n",
    "# Save metrics\n",
    "with open(exp_dir / 'metrics.json', 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "print(f\"\\nMetrics saved to {exp_dir / 'metrics.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Compare with Delta Hedging Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all baselines\n",
    "baseline_results = evaluate_all_baselines(S_test, v_test, Z_test, config)\n",
    "\n",
    "print(\"\\nBaseline Comparison:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Method':<25} {'Mean P&L':<12} {'Std P&L':<12} {'CVaR 5%':<12} {'Sharpe':<12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Deep Hedging results\n",
    "print(f\"{'Deep Hedging':<25} {metrics['pnl_mean']:<12.4f} {metrics['pnl_std']:<12.4f} {metrics['cvar_05']:<12.4f} {metrics['sharpe_ratio']:<12.4f}\")\n",
    "\n",
    "# Baselines\n",
    "for name, result in baseline_results.items():\n",
    "    print(f\"{name:<25} {result['mean_pnl']:<12.4f} {result['std_pnl']:<12.4f} {result['cvar_05']:<12.4f} {result['sharpe_ratio']:<12.4f}\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Visualize P&L Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Delta hedging P&L for comparison\n",
    "r = heston_config.get('r', 0.02)\n",
    "sigma = np.sqrt(heston_config.get('v_0', 0.0175))\n",
    "\n",
    "delta_baseline = DeltaHedgingBaseline(K, T, r, 'call')\n",
    "delta_deltas = delta_baseline.compute_deltas(S_test, v_test, sigma=sigma, dt=dt)\n",
    "\n",
    "# Compute delta hedging P&L manually\n",
    "c_prop = config['data'].get('transaction_cost', {}).get('c_prop', 0.001)\n",
    "delta_pnl = delta_baseline.compute_pnl(S_test, Z_test, delta_deltas, c_prop=c_prop)\n",
    "\n",
    "# Plot comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.hist(all_pnl, bins=100, alpha=0.7, label=f'Deep Hedging (CVaR={metrics[\"cvar_05\"]:.4f})', density=True)\n",
    "ax.hist(delta_pnl, bins=100, alpha=0.7, label=f'Delta Hedging (CVaR={np.percentile(delta_pnl, 5):.4f})', density=True)\n",
    "\n",
    "ax.axvline(metrics['cvar_05'], color='blue', linestyle='--', label='DH CVaR 5%')\n",
    "ax.axvline(np.percentile(delta_pnl, 5), color='orange', linestyle='--', label='Delta CVaR 5%')\n",
    "\n",
    "ax.set_xlabel('P&L')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('P&L Distribution: Deep Hedging vs Delta Hedging')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/baseline_pnl_distribution.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "Baseline model trained successfully. Key results:\n",
    "- Deep Hedging learns an optimal premium parameter\n",
    "- Outperforms Black-Scholes delta hedging on CVaR metric\n",
    "- Model ready for pruning experiments (Lottery Ticket Hypothesis)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_robust_lth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
