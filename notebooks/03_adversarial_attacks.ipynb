{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Attacks Evaluation\n",
    "\n",
    "Evaluate robustness of baseline and lottery tickets under adversarial attacks (FGSM, PGD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from src.utils.config import load_config, get_device\n",
    "from src.models.deep_hedging import create_model\n",
    "from src.models.losses import create_loss_function\n",
    "from src.data.heston import get_or_generate_dataset\n",
    "from src.data.preprocessor import create_dataloaders, compute_features\n",
    "from src.attacks.fgsm import create_fgsm_attack\n",
    "from src.attacks.pgd import create_pgd_attack\n",
    "from src.evaluation.metrics import compute_all_metrics, compute_robustness_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config('../configs/config.yaml')\n",
    "device = get_device(config)\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Extract key parameters\n",
    "heston_config = config['data']['heston']\n",
    "K = heston_config['K']\n",
    "T = config['data']['T']\n",
    "n_steps = config['data']['n_steps']\n",
    "dt = T / n_steps\n",
    "\n",
    "# Load test data\n",
    "cache_dir = config.get('caching', {}).get('directory', 'cache')\n",
    "S_test, v_test, Z_test = get_or_generate_dataset(config, 'test', cache_dir)\n",
    "\n",
    "# Create test dataloader\n",
    "batch_size = config.get('training', {}).get('batch_size', 256)\n",
    "_, _, test_loader = create_dataloaders(\n",
    "    S_test[:1000], v_test[:1000], Z_test[:1000],  # Dummy train/val\n",
    "    S_test[:1000], v_test[:1000], Z_test[:1000],\n",
    "    S_test, v_test, Z_test,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "print(f\"Test set: {S_test.shape[0]} paths\")\n",
    "print(f\"K={K}, T={T}, n_steps={n_steps}, dt={dt:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_robustness(model, loss_fn, test_loader, config, device):\n",
    "    \"\"\"\n",
    "    Evaluate model robustness against FGSM and PGD attacks.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with clean, FGSM, and PGD metrics\n",
    "    \"\"\"\n",
    "    heston_config = config['data']['heston']\n",
    "    K = heston_config['K']\n",
    "    T = config['data']['T']\n",
    "    n_steps = config['data']['n_steps']\n",
    "    dt = T / n_steps\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Create attacks\n",
    "    fgsm = create_fgsm_attack(model, loss_fn, config)\n",
    "    pgd = create_pgd_attack(model, loss_fn, config)\n",
    "    \n",
    "    clean_pnls, fgsm_pnls, pgd_pnls = [], [], []\n",
    "    \n",
    "    for S, v, Z in test_loader:\n",
    "        S, v, Z = S.to(device), v.to(device), Z.to(device)\n",
    "        features = compute_features(S, v, K, T, dt)\n",
    "        \n",
    "        # Clean evaluation\n",
    "        with torch.no_grad():\n",
    "            deltas, y = model(features, S)\n",
    "            pnl = loss_fn.compute_pnl(deltas, S, Z, dt)\n",
    "            clean_pnls.append(pnl.cpu())\n",
    "        \n",
    "        # FGSM attack\n",
    "        features_fgsm, _ = fgsm.attack(features, S, Z, dt)\n",
    "        with torch.no_grad():\n",
    "            deltas, y = model(features_fgsm, S)\n",
    "            pnl = loss_fn.compute_pnl(deltas, S, Z, dt)\n",
    "            fgsm_pnls.append(pnl.cpu())\n",
    "        \n",
    "        # PGD attack\n",
    "        features_pgd, _ = pgd.attack(features, S, Z, dt)\n",
    "        with torch.no_grad():\n",
    "            deltas, y = model(features_pgd, S)\n",
    "            pnl = loss_fn.compute_pnl(deltas, S, Z, dt)\n",
    "            pgd_pnls.append(pnl.cpu())\n",
    "    \n",
    "    clean_pnls = torch.cat(clean_pnls).numpy()\n",
    "    fgsm_pnls = torch.cat(fgsm_pnls).numpy()\n",
    "    pgd_pnls = torch.cat(pgd_pnls).numpy()\n",
    "    \n",
    "    # Compute metrics\n",
    "    clean_metrics = compute_all_metrics(clean_pnls)\n",
    "    fgsm_metrics = compute_all_metrics(fgsm_pnls)\n",
    "    pgd_metrics = compute_all_metrics(pgd_pnls)\n",
    "    \n",
    "    return {\n",
    "        'clean': clean_metrics,\n",
    "        'fgsm': fgsm_metrics,\n",
    "        'pgd': pgd_metrics,\n",
    "        'fgsm_gap': clean_metrics['pnl_mean'] - fgsm_metrics['pnl_mean'],\n",
    "        'pgd_gap': clean_metrics['pnl_mean'] - pgd_metrics['pnl_mean'],\n",
    "        'fgsm_cvar_gap': clean_metrics['cvar_05'] - fgsm_metrics['cvar_05'],\n",
    "        'pgd_cvar_gap': clean_metrics['cvar_05'] - pgd_metrics['cvar_05']\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_test = {}\n",
    "loss_fn = create_loss_function(config)\n",
    "\n",
    "# Baseline dense model\n",
    "baseline_path = Path('../experiments/baseline/checkpoints/best.pt')\n",
    "if baseline_path.exists():\n",
    "    model_dense = create_model(config)\n",
    "    checkpoint = torch.load(baseline_path, map_location=device)\n",
    "    model_dense.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model_dense = model_dense.to(device)\n",
    "    models_to_test['dense_baseline'] = model_dense\n",
    "    print(f\"Loaded dense baseline from {baseline_path}\")\n",
    "else:\n",
    "    print(f\"Dense baseline not found at {baseline_path}\")\n",
    "\n",
    "# Sparse tickets at different sparsity levels\n",
    "for sparsity in [50, 60, 70, 80, 90]:\n",
    "    ticket_path = Path(f'../experiments/pruning/sparsity_{sparsity}/checkpoints/best.pt')\n",
    "    if ticket_path.exists():\n",
    "        model_sparse = create_model(config)\n",
    "        checkpoint = torch.load(ticket_path, map_location=device)\n",
    "        model_sparse.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model_sparse = model_sparse.to(device)\n",
    "        models_to_test[f'ticket_{sparsity}%'] = model_sparse\n",
    "        print(f\"Loaded ticket {sparsity}% from {ticket_path}\")\n",
    "    else:\n",
    "        print(f\"Ticket {sparsity}% not found, skipping\")\n",
    "\n",
    "print(f\"\\nTotal models loaded: {len(models_to_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_attacks = {}\n",
    "\n",
    "for model_name, model in models_to_test.items():\n",
    "    print(f\"\\nEvaluating {model_name}...\")\n",
    "    \n",
    "    results = evaluate_robustness(model, loss_fn, test_loader, config, device)\n",
    "    results_attacks[model_name] = results\n",
    "    \n",
    "    print(f\"  Clean CVaR:      {results['clean']['cvar_05']:.6f}\")\n",
    "    print(f\"  FGSM CVaR:       {results['fgsm']['cvar_05']:.6f}\")\n",
    "    print(f\"  PGD CVaR:        {results['pgd']['cvar_05']:.6f}\")\n",
    "    print(f\"  PGD CVaR Gap:    {results['pgd_cvar_gap']:.6f}\")\n",
    "\n",
    "# Save results\n",
    "output_dir = Path('../experiments/adversarial')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(output_dir / 'attack_results.json', 'w') as f:\n",
    "    json.dump(results_attacks, f, indent=2, default=float)\n",
    "\n",
    "print(f\"\\nResults saved to {output_dir / 'attack_results.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"ADVERSARIAL ROBUSTNESS SUMMARY\")\n",
    "print(\"=\" * 90)\n",
    "print(f\"{'Model':<20} {'Clean CVaR':<15} {'FGSM CVaR':<15} {'PGD CVaR':<15} {'PGD Gap':<15}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for model_name, results in results_attacks.items():\n",
    "    print(f\"{model_name:<20} \"\n",
    "          f\"{results['clean']['cvar_05']:<15.4f} \"\n",
    "          f\"{results['fgsm']['cvar_05']:<15.4f} \"\n",
    "          f\"{results['pgd']['cvar_05']:<15.4f} \"\n",
    "          f\"{results['pgd_cvar_gap']:<15.4f}\")\n",
    "\n",
    "print(\"=\" * 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(results_attacks) > 0:\n",
    "    model_names = list(results_attacks.keys())\n",
    "    clean_cvars = [results_attacks[m]['clean']['cvar_05'] for m in model_names]\n",
    "    fgsm_cvars = [results_attacks[m]['fgsm']['cvar_05'] for m in model_names]\n",
    "    pgd_cvars = [results_attacks[m]['pgd']['cvar_05'] for m in model_names]\n",
    "    \n",
    "    x = np.arange(len(model_names))\n",
    "    width = 0.25\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    bars1 = ax.bar(x - width, clean_cvars, width, label='Clean', color='#2563eb', alpha=0.8)\n",
    "    bars2 = ax.bar(x, fgsm_cvars, width, label='FGSM Attack', color='#f59e0b', alpha=0.8)\n",
    "    bars3 = ax.bar(x + width, pgd_cvars, width, label='PGD Attack', color='#dc2626', alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Model')\n",
    "    ax.set_ylabel('CVaR 5%')\n",
    "    ax.set_title('Adversarial Robustness: Clean vs FGSM vs PGD')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(model_names, rotation=45, ha='right')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../figures/robustness_comparison.pdf', dpi=300)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No models to visualize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robustness gap vs sparsity\n",
    "if len(results_attacks) > 1:\n",
    "    # Extract sparsity levels from ticket names\n",
    "    sparsities = []\n",
    "    pgd_gaps = []\n",
    "    \n",
    "    for model_name, results in results_attacks.items():\n",
    "        if 'ticket' in model_name:\n",
    "            sparsity = int(model_name.split('_')[1].replace('%', ''))\n",
    "            sparsities.append(sparsity)\n",
    "            pgd_gaps.append(results['pgd_cvar_gap'])\n",
    "    \n",
    "    if sparsities:\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        \n",
    "        ax.plot(sparsities, pgd_gaps, 'o-', linewidth=2, markersize=8, color='#dc2626')\n",
    "        \n",
    "        # Add baseline gap if available\n",
    "        if 'dense_baseline' in results_attacks:\n",
    "            baseline_gap = results_attacks['dense_baseline']['pgd_cvar_gap']\n",
    "            ax.axhline(baseline_gap, color='#6b7280', linestyle='--', \n",
    "                      label=f'Dense Baseline ({baseline_gap:.4f})')\n",
    "        \n",
    "        ax.set_xlabel('Sparsity (%)')\n",
    "        ax.set_ylabel('PGD CVaR Gap')\n",
    "        ax.set_title('Robustness Degradation vs Sparsity')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('../figures/robustness_vs_sparsity.pdf', dpi=300)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Key findings:\n",
    "- Standard lottery tickets are vulnerable to adversarial attacks\n",
    "- Robustness gap tends to increase with sparsity\n",
    "- PGD attacks are more effective than FGSM\n",
    "- This motivates the need for adversarial training of sparse networks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_robust_lth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
