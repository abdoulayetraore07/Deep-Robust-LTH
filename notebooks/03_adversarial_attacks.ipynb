{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Attacks Evaluation\n",
    "\n",
    "Evaluate robustness of baseline and lottery tickets under adversarial attacks (FGSM, PGD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from src.utils.config import load_config, get_device\n",
    "from src.models.deep_hedging import create_model\n",
    "from src.models.losses import create_loss_function\n",
    "from src.data.heston import get_or_generate_dataset\n",
    "from src.data.preprocessor import create_dataloaders, compute_features\n",
    "from src.attacks.fgsm import create_fgsm_attack\n",
    "from src.attacks.pgd import create_pgd_attack\n",
    "from src.evaluation.metrics import compute_all_metrics, compute_robustness_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config('../configs/config.yaml')\n",
    "device = get_device(config)\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Extract key parameters\n",
    "heston_config = config['data']['heston']\n",
    "K = heston_config['K']\n",
    "T = config['data']['T']\n",
    "n_steps = config['data']['n_steps']\n",
    "dt = T / n_steps\n",
    "\n",
    "# Load test data\n",
    "cache_dir = config.get('caching', {}).get('directory', 'cache')\n",
    "S_test, v_test, Z_test = get_or_generate_dataset(config, 'test', cache_dir)\n",
    "\n",
    "# Create test dataloader\n",
    "batch_size = config.get('training', {}).get('batch_size', 256)\n",
    "_, _, test_loader = create_dataloaders(\n",
    "    S_test[:1000], v_test[:1000], Z_test[:1000],  # Dummy train/val\n",
    "    S_test[:1000], v_test[:1000], Z_test[:1000],\n",
    "    S_test, v_test, Z_test,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "print(f\"Test set: {S_test.shape[0]} paths\")\n",
    "print(f\"K={K}, T={T}, n_steps={n_steps}, dt={dt:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_robustness(model, loss_fn, test_loader, config, device):\n",
    "    \"\"\"\n",
    "    Evaluate model robustness against FGSM and PGD attacks.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with clean, FGSM, and PGD metrics\n",
    "    \"\"\"\n",
    "    heston_config = config['data']['heston']\n",
    "    K = heston_config['K']\n",
    "    T = config['data']['T']\n",
    "    n_steps = config['data']['n_steps']\n",
    "    dt = T / n_steps\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Create attacks\n",
    "    fgsm = create_fgsm_attack(model, loss_fn, config)\n",
    "    pgd = create_pgd_attack(model, loss_fn, config)\n",
    "    \n",
    "    clean_pnls, fgsm_pnls, pgd_pnls = [], [], []\n",
    "    \n",
    "    for S, v, Z in test_loader:\n",
    "        S, v, Z = S.to(device), v.to(device), Z.to(device)\n",
    "        features = compute_features(S, v, K, T, dt)\n",
    "        \n",
    "        # Clean evaluation\n",
    "        with torch.no_grad():\n",
    "            deltas, y = model(features, S)\n",
    "            pnl = loss_fn.compute_pnl(deltas, S, Z, dt)\n",
    "            clean_pnls.append(pnl.cpu())\n",
    "        \n",
    "        # FGSM attack\n",
    "        features_fgsm, _ = fgsm.attack(features, S, Z, dt)\n",
    "        with torch.no_grad():\n",
    "            deltas, y = model(features_fgsm, S)\n",
    "            pnl = loss_fn.compute_pnl(deltas, S, Z, dt)\n",
    "            fgsm_pnls.append(pnl.cpu())\n",
    "        \n",
    "        # PGD attack\n",
    "        features_pgd, _ = pgd.attack(features, S, Z, dt)\n",
    "        with torch.no_grad():\n",
    "            deltas, y = model(features_pgd, S)\n",
    "            pnl = loss_fn.compute_pnl(deltas, S, Z, dt)\n",
    "            pgd_pnls.append(pnl.cpu())\n",
    "    \n",
    "    clean_pnls = torch.cat(clean_pnls).numpy()\n",
    "    fgsm_pnls = torch.cat(fgsm_pnls).numpy()\n",
    "    pgd_pnls = torch.cat(pgd_pnls).numpy()\n",
    "    \n",
    "    # Compute metrics\n",
    "    clean_metrics = compute_all_metrics(clean_pnls)\n",
    "    fgsm_metrics = compute_all_metrics(fgsm_pnls)\n",
    "    pgd_metrics = compute_all_metrics(pgd_pnls)\n",
    "    \n",
    "    return {\n",
    "        'clean': clean_metrics,\n",
    "        'fgsm': fgsm_metrics,\n",
    "        'pgd': pgd_metrics,\n",
    "        'fgsm_gap': clean_metrics['pnl_mean'] - fgsm_metrics['pnl_mean'],\n",
    "        'pgd_gap': clean_metrics['pnl_mean'] - pgd_metrics['pnl_mean'],\n",
    "        'fgsm_cvar_gap': clean_metrics['cvar_05'] - fgsm_metrics['cvar_05'],\n",
    "        'pgd_cvar_gap': clean_metrics['cvar_05'] - pgd_metrics['cvar_05']\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_test = {}\n",
    "loss_fn = create_loss_function(config)\n",
    "\n",
    "# Baseline dense model\n",
    "baseline_path = Path('../experiments/baseline/checkpoints/best.pt')\n",
    "if baseline_path.exists():\n",
    "    model_dense = create_model(config)\n",
    "    checkpoint = torch.load(baseline_path, map_location=device)\n",
    "    model_dense.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model_dense = model_dense.to(device)\n",
    "    models_to_test['dense_baseline'] = model_dense\n",
    "    print(f\"Loaded dense baseline from {baseline_path}\")\n",
    "else:\n",
    "    print(f\"Dense baseline not found at {baseline_path}\")\n",
    "\n",
    "# Sparse tickets at different sparsity levels\n",
    "for sparsity in [50, 60, 70, 80, 90, 95]:\n",
    "    ticket_path = Path(f'../experiments/pruning/sparsity_{sparsity}/checkpoints/best.pt')\n",
    "    if ticket_path.exists():\n",
    "        model_sparse = create_model(config)\n",
    "        checkpoint = torch.load(ticket_path, map_location=device)\n",
    "        model_sparse.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model_sparse = model_sparse.to(device)\n",
    "        models_to_test[f'ticket_{sparsity}%'] = model_sparse\n",
    "        print(f\"Loaded ticket {sparsity}% from {ticket_path}\")\n",
    "    else:\n",
    "        print(f\"Ticket {sparsity}% not found, skipping\")\n",
    "\n",
    "print(f\"\\nTotal models loaded: {len(models_to_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_attacks = {}\n",
    "\n",
    "for model_name, model in models_to_test.items():\n",
    "    print(f\"\\nEvaluating {model_name}...\")\n",
    "    \n",
    "    results = evaluate_robustness(model, loss_fn, test_loader, config, device)\n",
    "    results_attacks[model_name] = results\n",
    "    \n",
    "    print(f\"  Clean CVaR:      {results['clean']['cvar_05']:.6f}\")\n",
    "    print(f\"  FGSM CVaR:       {results['fgsm']['cvar_05']:.6f}\")\n",
    "    print(f\"  PGD CVaR:        {results['pgd']['cvar_05']:.6f}\")\n",
    "    print(f\"  PGD CVaR Gap:    {results['pgd_cvar_gap']:.6f}\")\n",
    "\n",
    "# Save results\n",
    "output_dir = Path('../experiments/adversarial')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(output_dir / 'attack_results.json', 'w') as f:\n",
    "    json.dump(results_attacks, f, indent=2, default=float)\n",
    "\n",
    "print(f\"\\nResults saved to {output_dir / 'attack_results.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"ADVERSARIAL ROBUSTNESS SUMMARY\")\n",
    "print(\"=\" * 90)\n",
    "print(f\"{'Model':<20} {'Clean CVaR':<15} {'FGSM CVaR':<15} {'PGD CVaR':<15} {'PGD Gap':<15}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for model_name, results in results_attacks.items():\n",
    "    print(f\"{model_name:<20} \"\n",
    "          f\"{results['clean']['cvar_05']:<15.4f} \"\n",
    "          f\"{results['fgsm']['cvar_05']:<15.4f} \"\n",
    "          f\"{results['pgd']['cvar_05']:<15.4f} \"\n",
    "          f\"{results['pgd_cvar_gap']:<15.4f}\")\n",
    "\n",
    "print(\"=\" * 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(results_attacks) > 0:\n",
    "    model_names = list(results_attacks.keys())\n",
    "    clean_cvars = [results_attacks[m]['clean']['cvar_05'] for m in model_names]\n",
    "    fgsm_cvars = [results_attacks[m]['fgsm']['cvar_05'] for m in model_names]\n",
    "    pgd_cvars = [results_attacks[m]['pgd']['cvar_05'] for m in model_names]\n",
    "    \n",
    "    x = np.arange(len(model_names))\n",
    "    width = 0.25\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    bars1 = ax.bar(x - width, clean_cvars, width, label='Clean', color='#2563eb', alpha=0.8)\n",
    "    bars2 = ax.bar(x, fgsm_cvars, width, label='FGSM Attack', color='#f59e0b', alpha=0.8)\n",
    "    bars3 = ax.bar(x + width, pgd_cvars, width, label='PGD Attack', color='#dc2626', alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Model')\n",
    "    ax.set_ylabel('CVaR 5%')\n",
    "    ax.set_title('Adversarial Robustness: Clean vs FGSM vs PGD')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(model_names, rotation=45, ha='right')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../figures/robustness_comparison.pdf', dpi=300)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No models to visualize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robustness gap vs sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robustness gap vs sparsity\n",
    "if len(results_attacks) > 1:\n",
    "    # Extract sparsity levels from ticket names\n",
    "    sparsities = []\n",
    "    pgd_gaps = []\n",
    "    \n",
    "    for model_name, results in results_attacks.items():\n",
    "        if 'ticket' in model_name:\n",
    "            sparsity = int(model_name.split('_')[1].replace('%', ''))\n",
    "            sparsities.append(sparsity)\n",
    "            pgd_gaps.append(results['pgd_cvar_gap'])\n",
    "    \n",
    "    if sparsities:\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        \n",
    "        ax.plot(sparsities, pgd_gaps, 'o-', linewidth=2, markersize=8, color='#dc2626')\n",
    "        \n",
    "        # Add baseline gap if available\n",
    "        if 'dense_baseline' in results_attacks:\n",
    "            baseline_gap = results_attacks['dense_baseline']['pgd_cvar_gap']\n",
    "            ax.axhline(baseline_gap, color='#6b7280', linestyle='--', \n",
    "                      label=f'Dense Baseline ({baseline_gap:.4f})')\n",
    "        \n",
    "        ax.set_xlabel('Sparsity (%)')\n",
    "        ax.set_ylabel('PGD CVaR Gap')\n",
    "        ax.set_title('Robustness Degradation vs Sparsity')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('../figures/robustness_vs_sparsity.pdf', dpi=300)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trade-off Analysis: Clean Performance vs Robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FIGURE: Trade-off Curve (Clean Performance vs Robustness - Pareto Front)\n",
    "\n",
    "if len(results_attacks) > 1:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Extract data\n",
    "    model_names = list(results_attacks.keys())\n",
    "    clean_cvars = [results_attacks[m]['clean']['cvar_05'] for m in model_names]\n",
    "    pgd_cvars = [results_attacks[m]['pgd']['cvar_05'] for m in model_names]\n",
    "    pgd_gaps = [results_attacks[m]['pgd_cvar_gap'] for m in model_names]\n",
    "    \n",
    "    # Determine colors based on model type\n",
    "    colors = []\n",
    "    for name in model_names:\n",
    "        if 'dense' in name.lower() or 'baseline' in name.lower():\n",
    "            colors.append('#2563eb')  # Blue for dense\n",
    "        elif 'adv' in name.lower() or 'robust' in name.lower():\n",
    "            colors.append('#16a34a')  # Green for adversarially trained\n",
    "        else:\n",
    "            colors.append('#dc2626')  # Red for standard tickets\n",
    "    \n",
    "    # --- Plot 1: Clean CVaR vs PGD CVaR ---\n",
    "    ax1 = axes[0]\n",
    "    scatter = ax1.scatter(clean_cvars, pgd_cvars, c=colors, s=150, alpha=0.8, edgecolors='black', linewidth=1)\n",
    "    \n",
    "    # Add diagonal line (no degradation)\n",
    "    lims = [min(min(clean_cvars), min(pgd_cvars)) - 0.5, max(max(clean_cvars), max(pgd_cvars)) + 0.5]\n",
    "    ax1.plot(lims, lims, 'k--', alpha=0.3, label='No degradation')\n",
    "    \n",
    "    # Annotate points\n",
    "    for i, name in enumerate(model_names):\n",
    "        short_name = name.replace('_baseline', '').replace('ticket_', 't')\n",
    "        ax1.annotate(short_name, (clean_cvars[i], pgd_cvars[i]), \n",
    "                    textcoords=\"offset points\", xytext=(5, 5), fontsize=9)\n",
    "    \n",
    "    ax1.set_xlabel('Clean CVaR 5%', fontsize=11)\n",
    "    ax1.set_ylabel('PGD Attack CVaR 5%', fontsize=11)\n",
    "    ax1.set_title('Clean vs Adversarial Performance', fontsize=12, fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.legend(loc='lower right')\n",
    "    \n",
    "    # --- Plot 2: Radar/Spider Plot ---\n",
    "    ax2 = axes[1]\n",
    "    \n",
    "    # Select models for radar (max 5 for readability)\n",
    "    radar_models = model_names[:5] if len(model_names) > 5 else model_names\n",
    "    \n",
    "    # Metrics for radar\n",
    "    metrics_names = ['Clean CVaR', 'FGSM CVaR', 'PGD CVaR', 'Clean Sharpe', 'Robustness']\n",
    "    \n",
    "    # Normalize metrics to 0-1 scale (higher = better)\n",
    "    def normalize(values, higher_is_better=True):\n",
    "        min_v, max_v = min(values), max(values)\n",
    "        if max_v == min_v:\n",
    "            return [0.5] * len(values)\n",
    "        norm = [(v - min_v) / (max_v - min_v) for v in values]\n",
    "        return norm if higher_is_better else [1 - n for n in norm]\n",
    "    \n",
    "    radar_data = {}\n",
    "    for m in radar_models:\n",
    "        r = results_attacks[m]\n",
    "        # For CVaR, less negative is better\n",
    "        radar_data[m] = [\n",
    "            -r['clean']['cvar_05'],  # Negate so higher = better\n",
    "            -r['fgsm']['cvar_05'],\n",
    "            -r['pgd']['cvar_05'],\n",
    "            r['clean'].get('sharpe_ratio', 0),\n",
    "            -r['pgd_cvar_gap']  # Smaller gap = more robust = better\n",
    "        ]\n",
    "    \n",
    "    # Normalize each metric across models\n",
    "    for i in range(len(metrics_names)):\n",
    "        values = [radar_data[m][i] for m in radar_models]\n",
    "        norm_values = normalize(values)\n",
    "        for j, m in enumerate(radar_models):\n",
    "            radar_data[m][i] = norm_values[j]\n",
    "    \n",
    "    # Create radar chart\n",
    "    angles = np.linspace(0, 2 * np.pi, len(metrics_names), endpoint=False).tolist()\n",
    "    angles += angles[:1]  # Close the polygon\n",
    "    \n",
    "    radar_colors = ['#2563eb', '#dc2626', '#16a34a', '#9333ea', '#f59e0b']\n",
    "    \n",
    "    for idx, m in enumerate(radar_models):\n",
    "        values = radar_data[m] + radar_data[m][:1]\n",
    "        ax2.plot(angles, values, 'o-', linewidth=2, label=m, color=radar_colors[idx % len(radar_colors)])\n",
    "        ax2.fill(angles, values, alpha=0.1, color=radar_colors[idx % len(radar_colors)])\n",
    "    \n",
    "    ax2.set_xticks(angles[:-1])\n",
    "    ax2.set_xticklabels(metrics_names, fontsize=9)\n",
    "    ax2.set_ylim(0, 1)\n",
    "    ax2.set_title('Multi-Metric Comparison (Normalized)', fontsize=12, fontweight='bold')\n",
    "    ax2.legend(loc='upper right', bbox_to_anchor=(1.3, 1), fontsize=8)\n",
    "    \n",
    "    # Convert to polar\n",
    "    ax2.remove()\n",
    "    ax2 = fig.add_subplot(1, 2, 2, projection='polar')\n",
    "    \n",
    "    for idx, m in enumerate(radar_models):\n",
    "        values = radar_data[m] + radar_data[m][:1]\n",
    "        ax2.plot(angles, values, 'o-', linewidth=2, label=m, color=radar_colors[idx % len(radar_colors)])\n",
    "        ax2.fill(angles, values, alpha=0.15, color=radar_colors[idx % len(radar_colors)])\n",
    "    \n",
    "    ax2.set_xticks(angles[:-1])\n",
    "    ax2.set_xticklabels(metrics_names, fontsize=9)\n",
    "    ax2.set_ylim(0, 1)\n",
    "    ax2.set_title('Multi-Metric Comparison\\n(Normalized, outer=better)', fontsize=11, fontweight='bold')\n",
    "    ax2.legend(loc='upper right', bbox_to_anchor=(1.35, 1), fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../figures/adversarial_tradeoff_analysis.pdf', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Need multiple models for trade-off analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "Key findings:\n",
    "- Standard lottery tickets are vulnerable to adversarial attacks\n",
    "- Robustness gap tends to increase with sparsity\n",
    "- PGD attacks are more effective than FGSM\n",
    "- This motivates the need for adversarial training of sparse networks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_robust_lth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
