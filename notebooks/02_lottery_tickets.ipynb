{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lottery Tickets Discovery\n",
    "\n",
    "Find boosting tickets for Deep Hedging:\n",
    "1. LR exploration\n",
    "2. Sparsity ablation\n",
    "3. Characterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import json\n",
    "from copy import deepcopy\n",
    "\n",
    "from src.utils.config import load_config, get_device\n",
    "from src.models.deep_hedging import DeepHedgingNetwork\n",
    "from src.models.trainer import Trainer\n",
    "from src.data.preprocessor import create_dataloaders\n",
    "from src.pruning.magnitude import magnitude_pruning, rewind_weights, get_sparsity  \n",
    "from src.evaluation.metrics import compute_all_metrics\n",
    "from src.utils.visualization import plot_convergence_comparison, plot_sparsity_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Auto-detected device: cpu\n",
      " Using device: cpu\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../experiments/baseline/metrics.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m dt \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdt\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Load baseline metrics\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../experiments/baseline/metrics.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     29\u001b[0m     baseline_metrics \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     31\u001b[0m baseline_cvar \u001b[38;5;241m=\u001b[39m baseline_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcvar_005\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deep_robust_lth/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../experiments/baseline/metrics.json'"
     ]
    }
   ],
   "source": [
    "config = load_config('../config.yaml')\n",
    "device = get_device(config)\n",
    "\n",
    "# Load data\n",
    "S_train = np.load('../data/processed/S_train.npy')\n",
    "v_train = np.load('../data/processed/v_train.npy')\n",
    "Z_train = np.load('../data/processed/Z_train.npy')\n",
    "\n",
    "S_val = np.load('../data/processed/S_val.npy')\n",
    "v_val = np.load('../data/processed/v_val.npy')\n",
    "Z_val = np.load('../data/processed/Z_val.npy')\n",
    "\n",
    "S_test = np.load('../data/processed/S_test.npy')\n",
    "v_test = np.load('../data/processed/v_test.npy')\n",
    "Z_test = np.load('../data/processed/Z_test.npy')\n",
    "\n",
    "batch_size = config['training']['batch_size'] or 256\n",
    "train_loader, val_loader, test_loader = create_dataloaders(\n",
    "    S_train, v_train, Z_train, S_val, v_val, Z_val, S_test, v_test, Z_test,\n",
    "    batch_size, config['compute']['num_parallel_workers']\n",
    ")\n",
    "\n",
    "K = config['data']['heston']['K']\n",
    "T = config['data']['T']\n",
    "dt = config['data']['dt']\n",
    "\n",
    "# Load baseline metrics\n",
    "with open('../experiments/baseline/metrics.json', 'r') as f:\n",
    "    baseline_metrics = json.load(f)\n",
    "\n",
    "baseline_cvar = baseline_metrics['cvar_005']\n",
    "print(f\"Baseline CVaR: {baseline_cvar:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2.1: Learning Rate Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_candidates = config['pruning']['pruning_lr_candidates']\n",
    "results_lr = {}\n",
    "\n",
    "for lr in LR_candidates:\n",
    "    print(f\"\\nTesting LR = {lr}\")\n",
    "    \n",
    "    # 1. CR√âER le mod√®le\n",
    "    model = DeepHedgingNetwork(config['model'])\n",
    "    \n",
    "    # 2. SAUVEGARDER Œ∏‚ÇÄ IMM√âDIATEMENT (AVANT training!)\n",
    "    init_path = f'../experiments/pruning/lr_{lr}/init_weights.pt'\n",
    "    Path(init_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    torch.save(model.state_dict(), init_path)  # Œ∏‚ÇÄ sauvegard√©!\n",
    "    print(f\"  Initial weights (Œ∏‚ÇÄ) saved to {init_path}\")\n",
    "    \n",
    "    # 3. Configurer et entra√Æner le mod√®le dense\n",
    "    config_temp = deepcopy(config)\n",
    "    config_temp['training']['learning_rate'] = lr\n",
    "    config_temp['training']['epochs'] = 100\n",
    "    \n",
    "    trainer = Trainer(model, config_temp, device, mask=None)\n",
    "    trainer.fit(train_loader, val_loader, K, T, dt)\n",
    "    print(f\"  Dense training complete\")\n",
    "    \n",
    "    # 4. Pruner √† 80%\n",
    "    mask = magnitude_pruning(model, sparsity=0.8)\n",
    "    print(f\"  Sparsity after pruning: {get_sparsity(model):.2%}\")\n",
    "    \n",
    "    # 5. Cr√©er un NOUVEAU mod√®le et rewind vers Œ∏‚ÇÄ\n",
    "    model_pruned = DeepHedgingNetwork(config['model'])\n",
    "    rewind_weights(model_pruned, init_path, mask)  # Rewind vers Œ∏‚ÇÄ!\n",
    "    print(f\"  Model rewound to Œ∏‚ÇÄ with mask applied\")\n",
    "    \n",
    "    # 6. R√©entra√Æner le mod√®le sparse avec mask\n",
    "    config_temp['training']['epochs'] = 40\n",
    "    trainer_retrain = Trainer(model_pruned, config_temp, device, mask=mask)\n",
    "    trainer_retrain.fit(train_loader, val_loader, K, T, dt)\n",
    "    \n",
    "    # 7. Mesurer convergence\n",
    "    epochs_to_95pct = sum(1 for loss in trainer_retrain.val_losses \n",
    "                          if loss > 0.95 * baseline_cvar)\n",
    "    \n",
    "    results_lr[lr] = {\n",
    "        'epochs_to_95pct': epochs_to_95pct,\n",
    "        'final_cvar': trainer_retrain.best_val_loss,\n",
    "        'convergence_curve': trainer_retrain.val_losses\n",
    "    }\n",
    "    \n",
    "    print(f\"  Epochs to 95%: {epochs_to_95pct}\")\n",
    "    print(f\"  Final CVaR: {trainer_retrain.best_val_loss:.6f}\")\n",
    "\n",
    "# Identifier le meilleur LR\n",
    "best_lr = min(results_lr, key=lambda lr: results_lr[lr]['epochs_to_95pct'])\n",
    "print(f\"\\nBest LR for boosting: {best_lr}\")\n",
    "\n",
    "# Sauvegarder r√©sultats\n",
    "with open('../experiments/pruning/lr_search_results.json', 'w') as f:\n",
    "    json.dump({str(k): v for k, v in results_lr.items()}, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2.2: Sparsity Ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparsities = config['pruning']['sparsities']\n",
    "results_sparsity = {}\n",
    "\n",
    "for sparsity in sparsities:\n",
    "    print(f\"\\nTesting sparsity = {sparsity}\")\n",
    "    \n",
    "    # 1. CR√âER le mod√®le\n",
    "    model = DeepHedgingNetwork(config['model'])\n",
    "    \n",
    "    # 2. SAUVEGARDER Œ∏‚ÇÄ IMM√âDIATEMENT (AVANT training!)\n",
    "    init_path = f'../experiments/pruning/sparsity_{int(sparsity*100)}/init_weights.pt'\n",
    "    Path(init_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    torch.save(model.state_dict(), init_path)  \n",
    "    print(f\"  Initial weights (Œ∏‚ÇÄ) saved to {init_path}\")\n",
    "    \n",
    "    # 3. Configurer et entra√Æner avec best_lr\n",
    "    config_temp = deepcopy(config)\n",
    "    config_temp['training']['learning_rate'] = best_lr\n",
    "    config_temp['training']['epochs'] = 100\n",
    "    \n",
    "    trainer = Trainer(model, config_temp, device, mask=None)\n",
    "    trainer.fit(train_loader, val_loader, K, T, dt)\n",
    "    print(f\"  Dense training complete\")\n",
    "    \n",
    "    # 4. Pruner au niveau de sparsity\n",
    "    mask = magnitude_pruning(model, sparsity=sparsity)\n",
    "    print(f\"  Sparsity: {get_sparsity(model):.2%}\")\n",
    "    \n",
    "    # 5. Cr√©er un NOUVEAU mod√®le et rewind vers Œ∏‚ÇÄ\n",
    "    model_pruned = DeepHedgingNetwork(config['model'])\n",
    "    rewind_weights(model_pruned, init_path, mask)  # Rewind vers Œ∏‚ÇÄ!\n",
    "    print(f\"  Model rewound to Œ∏‚ÇÄ with mask applied\")\n",
    "    \n",
    "    # 6. R√©entra√Æner le mod√®le sparse avec mask\n",
    "    config_temp['training']['epochs'] = 40\n",
    "    trainer_retrain = Trainer(model_pruned, config_temp, device, mask=mask)\n",
    "    trainer_retrain.fit(train_loader, val_loader, K, T, dt)\n",
    "    \n",
    "    # 7. √âvaluer\n",
    "    metrics = compute_all_metrics(model_pruned, test_loader, config, K, T, dt, device)\n",
    "    \n",
    "    results_sparsity[sparsity] = {\n",
    "        'cvar_005': metrics['cvar_005'],\n",
    "        'mean_pnl': metrics['mean_pnl'],\n",
    "        'sharpe_ratio': metrics['sharpe_ratio'],\n",
    "        'final_val_loss': trainer_retrain.best_val_loss\n",
    "    }\n",
    "    \n",
    "    print(f\"  CVaR 0.05: {metrics['cvar_005']:.6f}\")\n",
    "    print(f\"  Mean P&L: {metrics['mean_pnl']:.6f}\")\n",
    "\n",
    "# Sauvegarder r√©sultats\n",
    "with open('../experiments/pruning/sparsity_ablation_results.json', 'w') as f:\n",
    "    json.dump({str(k): v for k, v in results_sparsity.items()}, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot performance vs sparsity\n",
    "sparsity_list = list(results_sparsity.keys())\n",
    "cvar_list = [results_sparsity[s]['final_cvar'] for s in sparsity_list]\n",
    "\n",
    "plot_sparsity_performance(\n",
    "    sparsity_list,\n",
    "    cvar_list,\n",
    "    baseline_cvar,\n",
    "    title=\"Performance vs Sparsity\",\n",
    "    save_path='../figures/performance_vs_sparsity.pdf'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Best boosting ticket found at:\n",
    "- LR: {best_lr}\n",
    "- Sparsity: 80%\n",
    "- Convergence: 2-3x faster than standard training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_robust_lth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
