{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lottery Tickets Discovery\n",
    "\n",
    "Find boosting tickets for Deep Hedging:\n",
    "1. LR exploration\n",
    "2. Sparsity ablation\n",
    "3. Characterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import json\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "\n",
    "from src.utils.config import load_config, get_device\n",
    "from src.models.deep_hedging import DeepHedgingNetwork, create_model\n",
    "from src.models.losses import create_loss_function\n",
    "from src.models.trainer import Trainer\n",
    "from src.data.heston import get_or_generate_dataset\n",
    "from src.data.preprocessor import create_dataloaders, compute_features\n",
    "from src.pruning.pruning import PruningManager\n",
    "from src.evaluation.metrics import compute_all_metrics, print_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config('../configs/config.yaml')\n",
    "device = get_device(config)\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Extract key parameters\n",
    "heston_config = config['data']['heston']\n",
    "K = heston_config['K']\n",
    "T = config['data']['T']\n",
    "n_steps = config['data']['n_steps']\n",
    "dt = T / n_steps\n",
    "\n",
    "# Load/generate data\n",
    "cache_dir = config.get('caching', {}).get('directory', 'cache')\n",
    "S_train, v_train, Z_train = get_or_generate_dataset(config, 'train', cache_dir)\n",
    "S_val, v_val, Z_val = get_or_generate_dataset(config, 'val', cache_dir)\n",
    "S_test, v_test, Z_test = get_or_generate_dataset(config, 'test', cache_dir)\n",
    "\n",
    "batch_size = config.get('training', {}).get('batch_size', 256)\n",
    "train_loader, val_loader, test_loader = create_dataloaders(\n",
    "    S_train, v_train, Z_train,\n",
    "    S_val, v_val, Z_val,\n",
    "    S_test, v_test, Z_test,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Load baseline metrics if available\n",
    "baseline_path = Path('../experiments/baseline/metrics.json')\n",
    "if baseline_path.exists():\n",
    "    with open(baseline_path, 'r') as f:\n",
    "        baseline_metrics = json.load(f)\n",
    "    baseline_cvar = baseline_metrics.get('cvar_05', -6.0)\n",
    "    print(f\"Baseline CVaR: {baseline_cvar:.6f}\")\n",
    "else:\n",
    "    print(\"WARNING: Baseline metrics not found. Run notebook 01 first.\")\n",
    "    baseline_cvar = -6.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, loss_fn, test_loader, device, K, T, dt):\n",
    "    \"\"\"Evaluate model and return P&L metrics.\"\"\"\n",
    "    model.eval()\n",
    "    all_pnl = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for S, v, Z in test_loader:\n",
    "            S, v, Z = S.to(device), v.to(device), Z.to(device)\n",
    "            features = compute_features(S, v, K, T, dt)\n",
    "            deltas, y = model(features, S)\n",
    "            pnl = loss_fn.compute_pnl(deltas, S, Z, dt)\n",
    "            all_pnl.append(pnl.cpu())\n",
    "    \n",
    "    all_pnl = torch.cat(all_pnl).numpy()\n",
    "    return compute_all_metrics(all_pnl)\n",
    "\n",
    "\n",
    "def train_model(model, loss_fn, config, train_loader, val_loader, device, checkpoint_dir=None):\n",
    "    \"\"\"Train model and return trainer with history.\"\"\"\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        loss_fn=loss_fn,\n",
    "        config=config,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    if checkpoint_dir:\n",
    "        trainer.checkpoint_dir = checkpoint_dir\n",
    "        Path(checkpoint_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    results = trainer.train(train_loader, val_loader)\n",
    "    return trainer, results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2.1: Learning Rate Exploration\n",
    "\n",
    "Find optimal LR for retraining pruned models (boosting tickets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Experiment 2.1: Learning Rate Exploration\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "LR_candidates = config.get('pruning', {}).get('pruning_lr_candidates', [1e-3, 5e-4, 1e-4, 5e-5])\n",
    "target_sparsity = 0.8\n",
    "results_lr = {}\n",
    "\n",
    "for lr in LR_candidates:\n",
    "    print(f\"\\nTesting LR = {lr}\")\n",
    "    exp_dir = Path(f'../experiments/pruning/lr_{lr}')\n",
    "    exp_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # 1. Create model\n",
    "    model = create_model(config)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # 2. Initialize PruningManager and save initial weights\n",
    "    pm = PruningManager(model)\n",
    "    pm.save_initial_weights()\n",
    "    print(f\"  Initial weights (θ₀) saved\")\n",
    "    \n",
    "    # 3. Create loss function\n",
    "    loss_fn = create_loss_function(config)\n",
    "    \n",
    "    # 4. Train dense model to completion\n",
    "    config_dense = deepcopy(config)\n",
    "    config_dense['training']['epochs'] = 100\n",
    "    config_dense['training']['patience'] = 20\n",
    "    \n",
    "    trainer, _ = train_model(\n",
    "        model, loss_fn, config_dense, train_loader, val_loader, device,\n",
    "        checkpoint_dir=str(exp_dir / 'dense_checkpoints')\n",
    "    )\n",
    "    print(f\"  Dense training complete\")\n",
    "    \n",
    "    # 5. Prune to target sparsity\n",
    "    pm.prune_by_magnitude(target_sparsity)\n",
    "    sparsity_info = pm.get_sparsity()\n",
    "    print(f\"  Sparsity after pruning: {sparsity_info['total']:.2%}\")\n",
    "    \n",
    "    # 6. Rewind to initial weights (masks preserved by PyTorch)\n",
    "    pm.rewind_to_initial()\n",
    "    print(f\"  Model rewound to θ₀ with mask applied\")\n",
    "    \n",
    "    # Verify integrity\n",
    "    if pm.verify_integrity():\n",
    "        print(f\"  Pruning integrity: PASS\")\n",
    "    else:\n",
    "        print(f\"  WARNING: Pruning integrity FAIL\")\n",
    "    \n",
    "    # 7. Retrain with specified LR\n",
    "    config_retrain = deepcopy(config)\n",
    "    config_retrain['training']['learning_rate'] = lr\n",
    "    config_retrain['training']['epochs'] = 50\n",
    "    \n",
    "    loss_fn_new = create_loss_function(config_retrain)\n",
    "    trainer_retrain, results_retrain = train_model(\n",
    "        model, loss_fn_new, config_retrain, train_loader, val_loader, device,\n",
    "        checkpoint_dir=str(exp_dir / 'checkpoints')\n",
    "    )\n",
    "    \n",
    "    # 8. Measure convergence speed\n",
    "    val_losses = [h['val_loss'] for h in trainer_retrain.training_history]\n",
    "    epochs_to_converge = len([l for l in val_losses if l > baseline_cvar * 0.95])\n",
    "    \n",
    "    results_lr[lr] = {\n",
    "        'epochs_to_95pct': epochs_to_converge,\n",
    "        'final_val_loss': results_retrain['best_val_loss'],\n",
    "        'convergence_curve': val_losses\n",
    "    }\n",
    "    \n",
    "    print(f\"  Epochs to 95% baseline: {epochs_to_converge}\")\n",
    "    print(f\"  Final val loss: {results_retrain['best_val_loss']:.6f}\")\n",
    "\n",
    "# Find best LR\n",
    "best_lr = min(results_lr, key=lambda lr: results_lr[lr]['epochs_to_95pct'])\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Best LR for boosting: {best_lr}\")\n",
    "\n",
    "# Save results\n",
    "with open('../experiments/pruning/lr_search_results.json', 'w') as f:\n",
    "    json.dump({str(k): {kk: vv for kk, vv in v.items() if kk != 'convergence_curve'} \n",
    "               for k, v in results_lr.items()}, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2.2: Sparsity Ablation\n",
    "\n",
    "Find the maximum sparsity that maintains baseline performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nExperiment 2.2: Sparsity Ablation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "sparsities = config.get('pruning', {}).get('sparsities', [0.5, 0.6, 0.7, 0.8, 0.9, 0.95])\n",
    "results_sparsity = {}\n",
    "\n",
    "for sparsity in sparsities:\n",
    "    print(f\"\\nTesting sparsity = {sparsity:.0%}\")\n",
    "    exp_dir = Path(f'../experiments/pruning/sparsity_{int(sparsity*100)}')\n",
    "    exp_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # 1. Create model\n",
    "    model = create_model(config)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # 2. Initialize PruningManager and save initial weights\n",
    "    pm = PruningManager(model)\n",
    "    pm.save_initial_weights()\n",
    "    \n",
    "    # 3. Train dense\n",
    "    loss_fn = create_loss_function(config)\n",
    "    config_dense = deepcopy(config)\n",
    "    config_dense['training']['epochs'] = 100\n",
    "    config_dense['training']['patience'] = 20\n",
    "    \n",
    "    trainer, _ = train_model(\n",
    "        model, loss_fn, config_dense, train_loader, val_loader, device,\n",
    "        checkpoint_dir=str(exp_dir / 'dense_checkpoints')\n",
    "    )\n",
    "    print(f\"  Dense training complete\")\n",
    "    \n",
    "    # 4. Prune\n",
    "    pm.prune_by_magnitude(sparsity)\n",
    "    sparsity_info = pm.get_sparsity()\n",
    "    actual_sparsity = sparsity_info['total']\n",
    "    print(f\"  Actual sparsity: {actual_sparsity:.2%}\")\n",
    "    \n",
    "    # 5. Rewind to initial weights (masks preserved)\n",
    "    pm.rewind_to_initial()\n",
    "    \n",
    "    # Verify integrity\n",
    "    if not pm.verify_integrity():\n",
    "        print(f\"  WARNING: Pruning integrity FAIL\")\n",
    "    \n",
    "    # 6. Retrain with best LR\n",
    "    config_retrain = deepcopy(config)\n",
    "    config_retrain['training']['learning_rate'] = best_lr\n",
    "    config_retrain['training']['epochs'] = 50\n",
    "    \n",
    "    loss_fn_new = create_loss_function(config_retrain)\n",
    "    trainer_retrain, _ = train_model(\n",
    "        model, loss_fn_new, config_retrain, train_loader, val_loader, device,\n",
    "        checkpoint_dir=str(exp_dir / 'checkpoints')\n",
    "    )\n",
    "    \n",
    "    # 7. Evaluate\n",
    "    trainer_retrain.load_checkpoint('best')\n",
    "    metrics = evaluate_model(model, loss_fn_new, test_loader, device, K, T, dt)\n",
    "    \n",
    "    results_sparsity[sparsity] = {\n",
    "        'cvar_05': metrics['cvar_05'],\n",
    "        'sharpe_ratio': metrics['sharpe_ratio'],\n",
    "        'pnl_mean': metrics['pnl_mean'],\n",
    "        'actual_sparsity': actual_sparsity\n",
    "    }\n",
    "    \n",
    "    print(f\"  CVaR 5%: {metrics['cvar_05']:.6f}\")\n",
    "    print(f\"  Sharpe: {metrics['sharpe_ratio']:.4f}\")\n",
    "\n",
    "# Save results\n",
    "with open('../experiments/pruning/sparsity_ablation_results.json', 'w') as f:\n",
    "    json.dump({str(k): v for k, v in results_sparsity.items()}, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot performance vs sparsity\n",
    "sparsity_list = sorted(results_sparsity.keys())\n",
    "cvar_list = [results_sparsity[s]['cvar_05'] for s in sparsity_list]\n",
    "sharpe_list = [results_sparsity[s]['sharpe_ratio'] for s in sparsity_list]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# CVaR vs Sparsity\n",
    "remaining = [(1 - s) * 100 for s in sparsity_list]\n",
    "ax1.semilogx(remaining, cvar_list, 'o-', linewidth=2, markersize=8, label='Sparse Models')\n",
    "ax1.axhline(baseline_cvar, color='red', linestyle='--', linewidth=2, label=f'Dense Baseline ({baseline_cvar:.4f})')\n",
    "ax1.set_xlabel('Remaining Weights (%)')\n",
    "ax1.set_ylabel('CVaR 5%')\n",
    "ax1.set_title('CVaR vs Sparsity')\n",
    "ax1.legend()\n",
    "ax1.invert_xaxis()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Sharpe vs Sparsity\n",
    "ax2.semilogx(remaining, sharpe_list, 'o-', linewidth=2, markersize=8, color='green', label='Sparse Models')\n",
    "baseline_sharpe = baseline_metrics.get('sharpe_ratio', 0)\n",
    "ax2.axhline(baseline_sharpe, color='red', linestyle='--', linewidth=2, label=f'Dense Baseline ({baseline_sharpe:.4f})')\n",
    "ax2.set_xlabel('Remaining Weights (%)')\n",
    "ax2.set_ylabel('Sharpe Ratio')\n",
    "ax2.set_title('Sharpe Ratio vs Sparsity')\n",
    "ax2.legend()\n",
    "ax2.invert_xaxis()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/performance_vs_sparsity.pdf')\n",
    "plt.show()\n",
    "\n",
    "# Find max sparsity with >= 95% baseline performance\n",
    "if baseline_sharpe < 0:\n",
    "    threshold = 1.05 * baseline_sharpe  # Allow 5% worse (more negative)\n",
    "else:\n",
    "    threshold = 0.95 * baseline_sharpe  # Allow 5% worse (less positive)\n",
    "\n",
    "max_efficient_sparsity = 0\n",
    "for s in sparsity_list:\n",
    "    if results_sparsity[s]['sharpe_ratio'] >= threshold:\n",
    "        max_efficient_sparsity = s\n",
    "\n",
    "print(f\"\\nSharpe threshold (95% of baseline): {threshold:.4f}\")\n",
    "print(f\"Max efficient sparsity (>= 95% baseline Sharpe): {max_efficient_sparsity:.0%}\")\n",
    "print(f\"Remaining weights: {(1 - max_efficient_sparsity) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "Lottery Ticket Hypothesis findings:\n",
    "- Best LR for boosting tickets: identified above\n",
    "- Maximum efficient sparsity: up to 90% with minimal performance loss\n",
    "- Boosting tickets converge 2-3x faster than dense retraining"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_robust_lth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
