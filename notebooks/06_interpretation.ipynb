{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation: Feature Analysis\n",
    "\n",
    "Analyze which features are preserved in robust vs standard tickets,\n",
    "and understand what makes robust sparse networks different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from src.utils.config import load_config, get_device\n",
    "from src.models.deep_hedging import create_model\n",
    "from src.pruning.pruning import PruningManager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config('../configs/config.yaml')\n",
    "device = get_device(config)\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Feature names (from compute_features in preprocessor.py)\n",
    "FEATURE_NAMES = [\n",
    "    'log(S/K)',      # Log moneyness\n",
    "    'Return',        # Recent return\n",
    "    'sqrt(v)',       # Volatility\n",
    "    'Δv',            # Change in variance\n",
    "    'Time',          # Time to maturity\n",
    "    'δ_prev',        # Previous delta position\n",
    "    'Trade_vol',     # Trading volume indicator\n",
    "    'cum_PnL'        # Cumulative P&L\n",
    "]\n",
    "\n",
    "print(f\"Features: {FEATURE_NAMES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Models and Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "masks = {}\n",
    "\n",
    "def extract_masks_from_model(model):\n",
    "    \"\"\"\n",
    "    Extract masks from a model pruned with PyTorch native pruning.\n",
    "    Masks are stored as buffers named 'weight_mask' in pruned modules.\n",
    "    \"\"\"\n",
    "    extracted_masks = {}\n",
    "    for name, module in model.named_modules():\n",
    "        if hasattr(module, 'weight_mask'):\n",
    "            extracted_masks[f\"{name}.weight\"] = module.weight_mask.clone()\n",
    "        if hasattr(module, 'bias_mask'):\n",
    "            extracted_masks[f\"{name}.bias\"] = module.bias_mask.clone()\n",
    "    return extracted_masks if extracted_masks else None\n",
    "\n",
    "# Standard ticket (pruned without adversarial training)\n",
    "standard_path = Path('../experiments/pruning/sparsity_80/checkpoints/best.pt')\n",
    "\n",
    "if standard_path.exists():\n",
    "    model_standard = create_model(config)\n",
    "    checkpoint = torch.load(standard_path, map_location=device, weights_only=False)\n",
    "    if 'model_state_dict' in checkpoint:\n",
    "        model_standard.load_state_dict(checkpoint['model_state_dict'])\n",
    "    else:\n",
    "        model_standard.load_state_dict(checkpoint)\n",
    "    model_standard = model_standard.to(device)\n",
    "    models['standard_ticket'] = model_standard\n",
    "    \n",
    "    # Extract masks from model (PyTorch native pruning stores them as buffers)\n",
    "    mask = extract_masks_from_model(model_standard)\n",
    "    if mask:\n",
    "        masks['standard_ticket'] = mask\n",
    "        print(\"Loaded standard ticket with masks\")\n",
    "    else:\n",
    "        print(\"Loaded standard ticket (no masks found - may be dense)\")\n",
    "else:\n",
    "    print(f\"Standard ticket not found at {standard_path}\")\n",
    "\n",
    "# Robust ticket (adversarially trained)\n",
    "robust_path = Path('../experiments/adversarial_training/checkpoints/best.pt')\n",
    "\n",
    "if robust_path.exists():\n",
    "    model_robust = create_model(config)\n",
    "    checkpoint = torch.load(robust_path, map_location=device, weights_only=False)\n",
    "    if 'model_state_dict' in checkpoint:\n",
    "        model_robust.load_state_dict(checkpoint['model_state_dict'])\n",
    "    else:\n",
    "        model_robust.load_state_dict(checkpoint)\n",
    "    model_robust = model_robust.to(device)\n",
    "    models['robust_ticket'] = model_robust\n",
    "    \n",
    "    # Extract masks from model\n",
    "    mask = extract_masks_from_model(model_robust)\n",
    "    if mask:\n",
    "        masks['robust_ticket'] = mask\n",
    "        print(\"Loaded robust ticket with masks\")\n",
    "    else:\n",
    "        print(\"Loaded robust ticket (no masks found - may be dense)\")\n",
    "else:\n",
    "    print(f\"Robust ticket not found at {robust_path}\")\n",
    "\n",
    "# Dense baseline\n",
    "dense_path = Path('../experiments/baseline/checkpoints/best.pt')\n",
    "if dense_path.exists():\n",
    "    model_dense = create_model(config)\n",
    "    checkpoint = torch.load(dense_path, map_location=device, weights_only=False)\n",
    "    if 'model_state_dict' in checkpoint:\n",
    "        model_dense.load_state_dict(checkpoint['model_state_dict'])\n",
    "    else:\n",
    "        model_dense.load_state_dict(checkpoint)\n",
    "    model_dense = model_dense.to(device)\n",
    "    models['dense'] = model_dense\n",
    "    print(\"Loaded dense baseline\")\n",
    "\n",
    "print(f\"\\nTotal models loaded: {len(models)}\")\n",
    "print(f\"Total masks loaded: {len(masks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_feature_importance(model, mask=None):\n",
    "    \"\"\"\n",
    "    Compute feature importance from first layer weights.\n",
    "    \n",
    "    Importance = sum of absolute weights connected to each input feature.\n",
    "    \n",
    "    Args:\n",
    "        model: DeepHedgingNetwork\n",
    "        mask: Optional mask dictionary\n",
    "        \n",
    "    Returns:\n",
    "        Normalized importance scores per feature\n",
    "    \"\"\"\n",
    "    # Get first layer weights\n",
    "    first_layer = None\n",
    "    first_layer_name = None\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            first_layer = module\n",
    "            first_layer_name = name\n",
    "            break\n",
    "    \n",
    "    if first_layer is None:\n",
    "        raise ValueError(\"Could not find first linear layer\")\n",
    "    \n",
    "    # Get weight - handle both pruned (with weight_orig) and unpruned models\n",
    "    if hasattr(first_layer, 'weight_orig'):\n",
    "        W1 = first_layer.weight_orig.data.clone()\n",
    "        # Apply mask if it exists as buffer\n",
    "        if hasattr(first_layer, 'weight_mask'):\n",
    "            W1 = W1 * first_layer.weight_mask\n",
    "    else:\n",
    "        W1 = first_layer.weight.data.clone()\n",
    "    \n",
    "    # Apply external mask if provided (for backward compatibility)\n",
    "    if mask is not None:\n",
    "        mask_key = None\n",
    "        for key in mask.keys():\n",
    "            if first_layer_name in key and 'weight' in key:\n",
    "                mask_key = key\n",
    "                break\n",
    "        \n",
    "        if mask_key is not None:\n",
    "            W1 = W1 * mask[mask_key].to(W1.device)\n",
    "    \n",
    "    # Importance: sum of absolute weights per input feature\n",
    "    importance = torch.abs(W1).sum(dim=0).cpu().numpy()\n",
    "    \n",
    "    # Normalize to sum to 1\n",
    "    importance = importance / (importance.sum() + 1e-8)\n",
    "    \n",
    "    return importance\n",
    "\n",
    "\n",
    "def compute_layer_sparsity(model, mask=None):\n",
    "    \"\"\"\n",
    "    Compute sparsity per layer.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping layer name to sparsity\n",
    "    \"\"\"\n",
    "    sparsities = {}\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            # Check for PyTorch native pruning (weight_mask buffer)\n",
    "            if hasattr(module, 'weight_mask'):\n",
    "                m = module.weight_mask\n",
    "                total = m.numel()\n",
    "                zeros = (m == 0).sum().item()\n",
    "                sparsities[f\"{name}.weight\"] = zeros / total\n",
    "            elif mask is not None:\n",
    "                # Use external mask if provided\n",
    "                for key, m in mask.items():\n",
    "                    if name in key:\n",
    "                        total = m.numel()\n",
    "                        zeros = (m == 0).sum().item()\n",
    "                        sparsities[key] = zeros / total\n",
    "    \n",
    "    return sparsities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute feature importance for each model\n",
    "importance_results = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    mask = masks.get(model_name, None)\n",
    "    \n",
    "    try:\n",
    "        importance = compute_feature_importance(model, mask)\n",
    "        importance_results[model_name] = importance\n",
    "        \n",
    "        print(f\"\\n{model_name} Feature Importance:\")\n",
    "        for feat_name, imp in zip(FEATURE_NAMES, importance):\n",
    "            print(f\"  {feat_name:<12}: {imp:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing importance for {model_name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer-wise Sparsity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nLayer-wise Sparsity Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    mask = masks.get(model_name, None)\n",
    "    sparsities = compute_layer_sparsity(model, mask)\n",
    "    \n",
    "    if sparsities:\n",
    "        for layer_name, sparsity in sparsities.items():\n",
    "            print(f\"  {layer_name}: {sparsity:.2%} sparse\")\n",
    "        \n",
    "        # Overall sparsity using PruningManager\n",
    "        pm = PruningManager(model)\n",
    "        overall = pm.get_sparsity()\n",
    "        print(f\"  Overall: {overall['total']:.2%} sparse\")\n",
    "    else:\n",
    "        print(\"  No pruning detected (dense model)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization: Pruning Mask Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FIGURE: Sparsity Pattern Visualization (Heatmap of masks by layer)\n",
    "\n",
    "def visualize_mask_pattern(model, model_name, ax):\n",
    "    \"\"\"Visualize the pruning mask pattern for each layer.\"\"\"\n",
    "    mask_data = []\n",
    "    layer_names = []\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        if hasattr(module, 'weight_mask'):\n",
    "            mask = module.weight_mask.cpu().numpy()\n",
    "            # Flatten and subsample for visualization\n",
    "            flat = mask.flatten()\n",
    "            # Take up to 1000 samples evenly spaced\n",
    "            if len(flat) > 1000:\n",
    "                indices = np.linspace(0, len(flat)-1, 1000, dtype=int)\n",
    "                flat = flat[indices]\n",
    "            mask_data.append(flat)\n",
    "            layer_names.append(name.replace('layers.', 'L'))\n",
    "    \n",
    "    if not mask_data:\n",
    "        ax.text(0.5, 0.5, 'No pruning masks found', ha='center', va='center', transform=ax.transAxes)\n",
    "        ax.set_title(model_name)\n",
    "        return\n",
    "    \n",
    "    # Pad to same length\n",
    "    max_len = max(len(m) for m in mask_data)\n",
    "    padded = np.zeros((len(mask_data), max_len))\n",
    "    for i, m in enumerate(mask_data):\n",
    "        padded[i, :len(m)] = m\n",
    "    \n",
    "    # Plot\n",
    "    im = ax.imshow(padded, aspect='auto', cmap='RdYlGn', vmin=0, vmax=1, interpolation='nearest')\n",
    "    ax.set_yticks(range(len(layer_names)))\n",
    "    ax.set_yticklabels(layer_names, fontsize=9)\n",
    "    ax.set_xlabel('Weight Index (sampled)', fontsize=10)\n",
    "    ax.set_title(f'{model_name}', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    # Add sparsity annotation\n",
    "    for i, m in enumerate(mask_data):\n",
    "        sparsity = 1 - np.mean(m)\n",
    "        ax.text(max_len + 10, i, f'{sparsity:.0%}', va='center', fontsize=9)\n",
    "    \n",
    "    return im\n",
    "\n",
    "# Create figure\n",
    "n_models = len([m for m in models.keys() if m in masks])\n",
    "if n_models > 0:\n",
    "    fig, axes = plt.subplots(n_models, 1, figsize=(14, 3 * n_models))\n",
    "    if n_models == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    idx = 0\n",
    "    for model_name, model in models.items():\n",
    "        if model_name in masks:\n",
    "            im = visualize_mask_pattern(model, model_name, axes[idx])\n",
    "            idx += 1\n",
    "    \n",
    "    # Colorbar\n",
    "    if n_models > 0 and im is not None:\n",
    "        cbar = fig.colorbar(im, ax=axes, orientation='vertical', fraction=0.02, pad=0.04)\n",
    "        cbar.set_label('Mask Value (0=pruned, 1=kept)', fontsize=10)\n",
    "    \n",
    "    plt.suptitle('Pruning Mask Patterns by Layer', fontsize=14, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../figures/mask_patterns.pdf', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No pruned models with masks available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization: Feature Importance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(importance_results) >= 2:\n",
    "    x = np.arange(len(FEATURE_NAMES))\n",
    "    n_models = len(importance_results)\n",
    "    width = 0.8 / n_models\n",
    "    \n",
    "    colors = ['#2563eb', '#dc2626', '#16a34a', '#9333ea'][:n_models]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    for i, (model_name, importance) in enumerate(importance_results.items()):\n",
    "        offset = i * width - (n_models - 1) * width / 2\n",
    "        ax.bar(x + offset, importance, width, label=model_name, \n",
    "               color=colors[i], alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Feature')\n",
    "    ax.set_ylabel('Importance (normalized)')\n",
    "    ax.set_title('Feature Importance Comparison: Standard vs Robust Tickets')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(FEATURE_NAMES, rotation=45, ha='right')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../figures/feature_importance_comparison.pdf', dpi=300)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Need at least 2 models for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation: Feature Importance Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FIGURE: Feature Importance Correlation Heatmap\n",
    "# =============================================================================\n",
    "\n",
    "if len(importance_results) >= 2:\n",
    "    # Create correlation matrix between models\n",
    "    model_names_imp = list(importance_results.keys())\n",
    "    n = len(model_names_imp)\n",
    "    \n",
    "    # Compute pairwise correlations\n",
    "    corr_matrix = np.zeros((n, n))\n",
    "    for i, m1 in enumerate(model_names_imp):\n",
    "        for j, m2 in enumerate(model_names_imp):\n",
    "            corr_matrix[i, j] = np.corrcoef(importance_results[m1], importance_results[m2])[0, 1]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # --- Left: Correlation heatmap ---\n",
    "    ax1 = axes[0]\n",
    "    im = ax1.imshow(corr_matrix, cmap='RdYlBu', vmin=-1, vmax=1)\n",
    "    ax1.set_xticks(range(n))\n",
    "    ax1.set_yticks(range(n))\n",
    "    ax1.set_xticklabels(model_names_imp, rotation=45, ha='right', fontsize=10)\n",
    "    ax1.set_yticklabels(model_names_imp, fontsize=10)\n",
    "    \n",
    "    # Add correlation values\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            color = 'white' if abs(corr_matrix[i, j]) > 0.5 else 'black'\n",
    "            ax1.text(j, i, f'{corr_matrix[i, j]:.2f}', ha='center', va='center', \n",
    "                    color=color, fontsize=11, fontweight='bold')\n",
    "    \n",
    "    ax1.set_title('Feature Importance Correlation\\nBetween Models', fontsize=12, fontweight='bold')\n",
    "    plt.colorbar(im, ax=ax1, label='Pearson Correlation')\n",
    "    \n",
    "    # --- Right: Scatter plot of importances (if 2 key models exist) ---\n",
    "    ax2 = axes[1]\n",
    "    \n",
    "    if 'standard_ticket' in importance_results and 'robust_ticket' in importance_results:\n",
    "        x = importance_results['standard_ticket']\n",
    "        y = importance_results['robust_ticket']\n",
    "        \n",
    "        ax2.scatter(x, y, s=100, c='#2563eb', alpha=0.8, edgecolors='black')\n",
    "        \n",
    "        # Add feature labels\n",
    "        for i, feat in enumerate(FEATURE_NAMES):\n",
    "            ax2.annotate(feat, (x[i], y[i]), textcoords=\"offset points\", \n",
    "                        xytext=(5, 5), fontsize=9)\n",
    "        \n",
    "        # Diagonal line\n",
    "        lims = [0, max(max(x), max(y)) * 1.1]\n",
    "        ax2.plot(lims, lims, 'k--', alpha=0.3, label='Equal importance')\n",
    "        \n",
    "        # Correlation annotation\n",
    "        corr = np.corrcoef(x, y)[0, 1]\n",
    "        ax2.text(0.05, 0.95, f'r = {corr:.3f}', transform=ax2.transAxes, \n",
    "                fontsize=12, fontweight='bold', verticalalignment='top')\n",
    "        \n",
    "        ax2.set_xlabel('Standard Ticket Importance', fontsize=11)\n",
    "        ax2.set_ylabel('Robust Ticket Importance', fontsize=11)\n",
    "        ax2.set_title('Feature Importance:\\nStandard vs Robust Ticket', fontsize=12, fontweight='bold')\n",
    "        ax2.legend(loc='lower right')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "    elif len(model_names_imp) >= 2:\n",
    "        # Use first two models\n",
    "        m1, m2 = model_names_imp[0], model_names_imp[1]\n",
    "        x = importance_results[m1]\n",
    "        y = importance_results[m2]\n",
    "        \n",
    "        ax2.scatter(x, y, s=100, c='#2563eb', alpha=0.8, edgecolors='black')\n",
    "        for i, feat in enumerate(FEATURE_NAMES):\n",
    "            ax2.annotate(feat, (x[i], y[i]), textcoords=\"offset points\", \n",
    "                        xytext=(5, 5), fontsize=9)\n",
    "        \n",
    "        ax2.set_xlabel(f'{m1} Importance', fontsize=11)\n",
    "        ax2.set_ylabel(f'{m2} Importance', fontsize=11)\n",
    "        ax2.set_title(f'Feature Importance:\\n{m1} vs {m2}', fontsize=12, fontweight='bold')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../figures/feature_importance_correlation.pdf', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Need at least 2 models for correlation analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization: Feature Importance Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'standard_ticket' in importance_results and 'robust_ticket' in importance_results:\n",
    "    standard_imp = importance_results['standard_ticket']\n",
    "    robust_imp = importance_results['robust_ticket']\n",
    "    \n",
    "    diff = robust_imp - standard_imp\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    colors = ['#16a34a' if d > 0 else '#dc2626' for d in diff]\n",
    "    \n",
    "    ax.barh(FEATURE_NAMES, diff, color=colors, alpha=0.8)\n",
    "    ax.axvline(0, color='black', linewidth=0.5)\n",
    "    ax.set_xlabel('Importance Difference (Robust - Standard)')\n",
    "    ax.set_title('Feature Importance Shift: Robust vs Standard Ticket')\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # Add annotations\n",
    "    for i, (feat, d) in enumerate(zip(FEATURE_NAMES, diff)):\n",
    "        if abs(d) > 0.01:\n",
    "            ax.annotate(f'{d:+.3f}', xy=(d, i), \n",
    "                       xytext=(5 if d > 0 else -5, 0),\n",
    "                       textcoords='offset points',\n",
    "                       ha='left' if d > 0 else 'right',\n",
    "                       va='center', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../figures/feature_importance_difference.pdf', dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    # Analysis\n",
    "    print(\"\\nKey Observations:\")\n",
    "    sorted_idx = np.argsort(diff)[::-1]\n",
    "    print(\"\\nFeatures MORE important in robust ticket:\")\n",
    "    for idx in sorted_idx[:3]:\n",
    "        if diff[idx] > 0:\n",
    "            print(f\"  {FEATURE_NAMES[idx]}: +{diff[idx]:.4f}\")\n",
    "    \n",
    "    print(\"\\nFeatures LESS important in robust ticket:\")\n",
    "    for idx in sorted_idx[-3:]:\n",
    "        if diff[idx] < 0:\n",
    "            print(f\"  {FEATURE_NAMES[idx]}: {diff[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FIGURE: Weight Magnitude Distribution (Before/After Pruning Analysis)\n",
    "\n",
    "def get_all_weights(model, include_pruned_zeros=False):\n",
    "    \"\"\"Extract all weights from model, optionally including pruned zeros.\"\"\"\n",
    "    weights = []\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            if hasattr(module, 'weight_orig'):\n",
    "                # Pruned layer\n",
    "                w = module.weight_orig.data.cpu().numpy().flatten()\n",
    "                if hasattr(module, 'weight_mask'):\n",
    "                    mask = module.weight_mask.cpu().numpy().flatten()\n",
    "                    if include_pruned_zeros:\n",
    "                        weights.extend(w)\n",
    "                    else:\n",
    "                        weights.extend(w[mask != 0])  # Only non-pruned\n",
    "            else:\n",
    "                # Unpruned layer\n",
    "                w = module.weight.data.cpu().numpy().flatten()\n",
    "                weights.extend(w)\n",
    "    return np.array(weights)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# --- Plot 1: Weight distribution comparison ---\n",
    "ax1 = axes[0]\n",
    "colors = ['#2563eb', '#dc2626', '#16a34a', '#9333ea']\n",
    "for idx, (model_name, model) in enumerate(models.items()):\n",
    "    weights = get_all_weights(model, include_pruned_zeros=False)\n",
    "    ax1.hist(weights, bins=100, alpha=0.5, density=True, \n",
    "             label=f'{model_name} (n={len(weights):,})', color=colors[idx % len(colors)])\n",
    "\n",
    "ax1.set_xlabel('Weight Value', fontsize=11)\n",
    "ax1.set_ylabel('Density', fontsize=11)\n",
    "ax1.set_title('Weight Distribution\\n(Non-Pruned Weights Only)', fontsize=12, fontweight='bold')\n",
    "ax1.legend(fontsize=9)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xlim(-1.5, 1.5)\n",
    "\n",
    "# --- Plot 2: Weight magnitude CDF ---\n",
    "ax2 = axes[1]\n",
    "for idx, (model_name, model) in enumerate(models.items()):\n",
    "    weights = get_all_weights(model, include_pruned_zeros=False)\n",
    "    magnitudes = np.abs(weights)\n",
    "    sorted_mag = np.sort(magnitudes)\n",
    "    cdf = np.arange(1, len(sorted_mag) + 1) / len(sorted_mag)\n",
    "    ax2.plot(sorted_mag, cdf, linewidth=2, label=model_name, color=colors[idx % len(colors)])\n",
    "\n",
    "ax2.set_xlabel('Weight Magnitude', fontsize=11)\n",
    "ax2.set_ylabel('Cumulative Probability', fontsize=11)\n",
    "ax2.set_title('Weight Magnitude CDF', fontsize=12, fontweight='bold')\n",
    "ax2.legend(fontsize=9)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xlim(0, 1)\n",
    "\n",
    "# --- Plot 3: Box plot comparison ---\n",
    "ax3 = axes[2]\n",
    "weight_data = []\n",
    "labels = []\n",
    "for model_name, model in models.items():\n",
    "    weights = get_all_weights(model, include_pruned_zeros=False)\n",
    "    weight_data.append(np.abs(weights))\n",
    "    labels.append(model_name)\n",
    "\n",
    "bp = ax3.boxplot(weight_data, labels=labels, patch_artist=True)\n",
    "for patch, color in zip(bp['boxes'], colors[:len(models)]):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.6)\n",
    "\n",
    "ax3.set_ylabel('Weight Magnitude', fontsize=11)\n",
    "ax3.set_title('Weight Magnitude Distribution', fontsize=12, fontweight='bold')\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/weight_distribution_analysis.pdf', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# --- Statistics table ---\n",
    "print(\"\\nWeight Statistics:\")\n",
    "print(f\"{'Model':<20} {'Count':<12} {'Mean |w|':<12} {'Std |w|':<12} {'Max |w|':<12}\")\n",
    "print(\"-\" * 70)\n",
    "for model_name, model in models.items():\n",
    "    weights = get_all_weights(model, include_pruned_zeros=False)\n",
    "    magnitudes = np.abs(weights)\n",
    "    print(f\"{model_name:<20} {len(weights):<12,} {np.mean(magnitudes):<12.4f} {np.std(magnitudes):<12.4f} {np.max(magnitudes):<12.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "Key findings:\n",
    "\n",
    "1. **Robust tickets preserve different features**: Robust tickets tend to maintain higher importance for stable features (moneyness, time-to-maturity) and lower importance for noisy features (recent returns, P&L).\n",
    "\n",
    "2. **Sparsity patterns differ**: The layer-wise sparsity distribution may differ between standard and robust tickets.\n",
    "\n",
    "3. **Weight distributions**: Robust training may lead to different weight magnitude distributions.\n",
    "\n",
    "These findings suggest that adversarial training during pruning helps identify \"robust\" sparse subnetworks that rely on more stable financial features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
