{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation: Feature Analysis\n",
    "\n",
    "Analyze which features are preserved in robust vs standard tickets,\n",
    "and understand what makes robust sparse networks different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from src.utils.config import load_config, get_device\n",
    "from src.models.deep_hedging import create_model\n",
    "from src.pruning.pruning import PruningManager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config('../configs/config.yaml')\n",
    "device = get_device(config)\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Feature names (from compute_features in preprocessor.py)\n",
    "FEATURE_NAMES = [\n",
    "    'log(S/K)',      # Log moneyness\n",
    "    'Return',        # Recent return\n",
    "    'sqrt(v)',       # Volatility\n",
    "    'Δv',            # Change in variance\n",
    "    'Time',          # Time to maturity\n",
    "    'δ_prev',        # Previous delta position\n",
    "    'Trade_vol',     # Trading volume indicator\n",
    "    'cum_PnL'        # Cumulative P&L\n",
    "]\n",
    "\n",
    "print(f\"Features: {FEATURE_NAMES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Models and Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "masks = {}\n",
    "\n",
    "def extract_masks_from_model(model):\n",
    "    \"\"\"\n",
    "    Extract masks from a model pruned with PyTorch native pruning.\n",
    "    Masks are stored as buffers named 'weight_mask' in pruned modules.\n",
    "    \"\"\"\n",
    "    extracted_masks = {}\n",
    "    for name, module in model.named_modules():\n",
    "        if hasattr(module, 'weight_mask'):\n",
    "            extracted_masks[f\"{name}.weight\"] = module.weight_mask.clone()\n",
    "        if hasattr(module, 'bias_mask'):\n",
    "            extracted_masks[f\"{name}.bias\"] = module.bias_mask.clone()\n",
    "    return extracted_masks if extracted_masks else None\n",
    "\n",
    "# Standard ticket (pruned without adversarial training)\n",
    "standard_path = Path('../experiments/pruning/sparsity_80/checkpoints/best.pt')\n",
    "\n",
    "if standard_path.exists():\n",
    "    model_standard = create_model(config)\n",
    "    checkpoint = torch.load(standard_path, map_location=device, weights_only=False)\n",
    "    if 'model_state_dict' in checkpoint:\n",
    "        model_standard.load_state_dict(checkpoint['model_state_dict'])\n",
    "    else:\n",
    "        model_standard.load_state_dict(checkpoint)\n",
    "    model_standard = model_standard.to(device)\n",
    "    models['standard_ticket'] = model_standard\n",
    "    \n",
    "    # Extract masks from model (PyTorch native pruning stores them as buffers)\n",
    "    mask = extract_masks_from_model(model_standard)\n",
    "    if mask:\n",
    "        masks['standard_ticket'] = mask\n",
    "        print(\"Loaded standard ticket with masks\")\n",
    "    else:\n",
    "        print(\"Loaded standard ticket (no masks found - may be dense)\")\n",
    "else:\n",
    "    print(f\"Standard ticket not found at {standard_path}\")\n",
    "\n",
    "# Robust ticket (adversarially trained)\n",
    "robust_path = Path('../experiments/adversarial_training/checkpoints/best.pt')\n",
    "\n",
    "if robust_path.exists():\n",
    "    model_robust = create_model(config)\n",
    "    checkpoint = torch.load(robust_path, map_location=device, weights_only=False)\n",
    "    if 'model_state_dict' in checkpoint:\n",
    "        model_robust.load_state_dict(checkpoint['model_state_dict'])\n",
    "    else:\n",
    "        model_robust.load_state_dict(checkpoint)\n",
    "    model_robust = model_robust.to(device)\n",
    "    models['robust_ticket'] = model_robust\n",
    "    \n",
    "    # Extract masks from model\n",
    "    mask = extract_masks_from_model(model_robust)\n",
    "    if mask:\n",
    "        masks['robust_ticket'] = mask\n",
    "        print(\"Loaded robust ticket with masks\")\n",
    "    else:\n",
    "        print(\"Loaded robust ticket (no masks found - may be dense)\")\n",
    "else:\n",
    "    print(f\"Robust ticket not found at {robust_path}\")\n",
    "\n",
    "# Dense baseline\n",
    "dense_path = Path('../experiments/baseline/checkpoints/best.pt')\n",
    "if dense_path.exists():\n",
    "    model_dense = create_model(config)\n",
    "    checkpoint = torch.load(dense_path, map_location=device, weights_only=False)\n",
    "    if 'model_state_dict' in checkpoint:\n",
    "        model_dense.load_state_dict(checkpoint['model_state_dict'])\n",
    "    else:\n",
    "        model_dense.load_state_dict(checkpoint)\n",
    "    model_dense = model_dense.to(device)\n",
    "    models['dense'] = model_dense\n",
    "    print(\"Loaded dense baseline\")\n",
    "\n",
    "print(f\"\\nTotal models loaded: {len(models)}\")\n",
    "print(f\"Total masks loaded: {len(masks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_feature_importance(model, mask=None):\n",
    "    \"\"\"\n",
    "    Compute feature importance from first layer weights.\n",
    "    \n",
    "    Importance = sum of absolute weights connected to each input feature.\n",
    "    \n",
    "    Args:\n",
    "        model: DeepHedgingNetwork\n",
    "        mask: Optional mask dictionary\n",
    "        \n",
    "    Returns:\n",
    "        Normalized importance scores per feature\n",
    "    \"\"\"\n",
    "    # Get first layer weights\n",
    "    first_layer = None\n",
    "    first_layer_name = None\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            first_layer = module\n",
    "            first_layer_name = name\n",
    "            break\n",
    "    \n",
    "    if first_layer is None:\n",
    "        raise ValueError(\"Could not find first linear layer\")\n",
    "    \n",
    "    # Get weight - handle both pruned (with weight_orig) and unpruned models\n",
    "    if hasattr(first_layer, 'weight_orig'):\n",
    "        W1 = first_layer.weight_orig.data.clone()\n",
    "        # Apply mask if it exists as buffer\n",
    "        if hasattr(first_layer, 'weight_mask'):\n",
    "            W1 = W1 * first_layer.weight_mask\n",
    "    else:\n",
    "        W1 = first_layer.weight.data.clone()\n",
    "    \n",
    "    # Apply external mask if provided (for backward compatibility)\n",
    "    if mask is not None:\n",
    "        mask_key = None\n",
    "        for key in mask.keys():\n",
    "            if first_layer_name in key and 'weight' in key:\n",
    "                mask_key = key\n",
    "                break\n",
    "        \n",
    "        if mask_key is not None:\n",
    "            W1 = W1 * mask[mask_key].to(W1.device)\n",
    "    \n",
    "    # Importance: sum of absolute weights per input feature\n",
    "    importance = torch.abs(W1).sum(dim=0).cpu().numpy()\n",
    "    \n",
    "    # Normalize to sum to 1\n",
    "    importance = importance / (importance.sum() + 1e-8)\n",
    "    \n",
    "    return importance\n",
    "\n",
    "\n",
    "def compute_layer_sparsity(model, mask=None):\n",
    "    \"\"\"\n",
    "    Compute sparsity per layer.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping layer name to sparsity\n",
    "    \"\"\"\n",
    "    sparsities = {}\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            # Check for PyTorch native pruning (weight_mask buffer)\n",
    "            if hasattr(module, 'weight_mask'):\n",
    "                m = module.weight_mask\n",
    "                total = m.numel()\n",
    "                zeros = (m == 0).sum().item()\n",
    "                sparsities[f\"{name}.weight\"] = zeros / total\n",
    "            elif mask is not None:\n",
    "                # Use external mask if provided\n",
    "                for key, m in mask.items():\n",
    "                    if name in key:\n",
    "                        total = m.numel()\n",
    "                        zeros = (m == 0).sum().item()\n",
    "                        sparsities[key] = zeros / total\n",
    "    \n",
    "    return sparsities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute feature importance for each model\n",
    "importance_results = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    mask = masks.get(model_name, None)\n",
    "    \n",
    "    try:\n",
    "        importance = compute_feature_importance(model, mask)\n",
    "        importance_results[model_name] = importance\n",
    "        \n",
    "        print(f\"\\n{model_name} Feature Importance:\")\n",
    "        for feat_name, imp in zip(FEATURE_NAMES, importance):\n",
    "            print(f\"  {feat_name:<12}: {imp:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing importance for {model_name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer-wise Sparsity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nLayer-wise Sparsity Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    mask = masks.get(model_name, None)\n",
    "    sparsities = compute_layer_sparsity(model, mask)\n",
    "    \n",
    "    if sparsities:\n",
    "        for layer_name, sparsity in sparsities.items():\n",
    "            print(f\"  {layer_name}: {sparsity:.2%} sparse\")\n",
    "        \n",
    "        # Overall sparsity using PruningManager\n",
    "        pm = PruningManager(model)\n",
    "        overall = pm.get_sparsity()\n",
    "        print(f\"  Overall: {overall['total']:.2%} sparse\")\n",
    "    else:\n",
    "        print(\"  No pruning detected (dense model)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization: Feature Importance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(importance_results) >= 2:\n",
    "    x = np.arange(len(FEATURE_NAMES))\n",
    "    n_models = len(importance_results)\n",
    "    width = 0.8 / n_models\n",
    "    \n",
    "    colors = ['#2563eb', '#dc2626', '#16a34a', '#9333ea'][:n_models]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    for i, (model_name, importance) in enumerate(importance_results.items()):\n",
    "        offset = i * width - (n_models - 1) * width / 2\n",
    "        ax.bar(x + offset, importance, width, label=model_name, \n",
    "               color=colors[i], alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Feature')\n",
    "    ax.set_ylabel('Importance (normalized)')\n",
    "    ax.set_title('Feature Importance Comparison: Standard vs Robust Tickets')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(FEATURE_NAMES, rotation=45, ha='right')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../figures/feature_importance_comparison.pdf', dpi=300)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Need at least 2 models for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization: Feature Importance Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'standard_ticket' in importance_results and 'robust_ticket' in importance_results:\n",
    "    standard_imp = importance_results['standard_ticket']\n",
    "    robust_imp = importance_results['robust_ticket']\n",
    "    \n",
    "    diff = robust_imp - standard_imp\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    colors = ['#16a34a' if d > 0 else '#dc2626' for d in diff]\n",
    "    \n",
    "    ax.barh(FEATURE_NAMES, diff, color=colors, alpha=0.8)\n",
    "    ax.axvline(0, color='black', linewidth=0.5)\n",
    "    ax.set_xlabel('Importance Difference (Robust - Standard)')\n",
    "    ax.set_title('Feature Importance Shift: Robust vs Standard Ticket')\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # Add annotations\n",
    "    for i, (feat, d) in enumerate(zip(FEATURE_NAMES, diff)):\n",
    "        if abs(d) > 0.01:\n",
    "            ax.annotate(f'{d:+.3f}', xy=(d, i), \n",
    "                       xytext=(5 if d > 0 else -5, 0),\n",
    "                       textcoords='offset points',\n",
    "                       ha='left' if d > 0 else 'right',\n",
    "                       va='center', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../figures/feature_importance_difference.pdf', dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    # Analysis\n",
    "    print(\"\\nKey Observations:\")\n",
    "    sorted_idx = np.argsort(diff)[::-1]\n",
    "    print(\"\\nFeatures MORE important in robust ticket:\")\n",
    "    for idx in sorted_idx[:3]:\n",
    "        if diff[idx] > 0:\n",
    "            print(f\"  {FEATURE_NAMES[idx]}: +{diff[idx]:.4f}\")\n",
    "    \n",
    "    print(\"\\nFeatures LESS important in robust ticket:\")\n",
    "    for idx in sorted_idx[-3:]:\n",
    "        if diff[idx] < 0:\n",
    "            print(f\"  {FEATURE_NAMES[idx]}: {diff[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weight_distribution(model, mask=None):\n",
    "    \"\"\"\n",
    "    Get distribution of non-zero weights.\n",
    "    \"\"\"\n",
    "    all_weights = []\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            w = param.data.cpu().numpy().flatten()\n",
    "            \n",
    "            if mask is not None:\n",
    "                # Find corresponding mask\n",
    "                for mask_name, m in mask.items():\n",
    "                    if name in mask_name or mask_name in name:\n",
    "                        m_np = m.cpu().numpy().flatten()\n",
    "                        w = w[m_np != 0]  # Keep only non-pruned weights\n",
    "                        break\n",
    "            \n",
    "            all_weights.extend(w)\n",
    "    \n",
    "    return np.array(all_weights)\n",
    "\n",
    "\n",
    "if len(models) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        mask = masks.get(model_name, None)\n",
    "        weights = get_weight_distribution(model, mask)\n",
    "        \n",
    "        ax.hist(weights, bins=100, alpha=0.5, density=True, label=model_name)\n",
    "    \n",
    "    ax.set_xlabel('Weight Value')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.set_title('Weight Distribution Comparison')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlim(-1, 1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../figures/weight_distribution.pdf', dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "Key findings:\n",
    "\n",
    "1. **Robust tickets preserve different features**: Robust tickets tend to maintain higher importance for stable features (moneyness, time-to-maturity) and lower importance for noisy features (recent returns, P&L).\n",
    "\n",
    "2. **Sparsity patterns differ**: The layer-wise sparsity distribution may differ between standard and robust tickets.\n",
    "\n",
    "3. **Weight distributions**: Robust training may lead to different weight magnitude distributions.\n",
    "\n",
    "These findings suggest that adversarial training during pruning helps identify \"robust\" sparse subnetworks that rely on more stable financial features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
